{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da4d2e04-a3a1-4c96-aa9d-58b1980f9b28",
   "metadata": {},
   "source": [
    "## 1. 定义分词方法及获取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e326ec0-643b-46b9-bfdc-c282c83a479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import os\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "\n",
    "DATA_HOME = os.path.join(os.path.dirname(os.path.dirname('./')), 'data')\n",
    "\n",
    "def clean_str(string, sep=\" \"):\n",
    "    \"\"\"\n",
    "    该函数的作用是去掉一个字符串中的所有非中文字符\n",
    "    :param string: 输入必须是字符串类型\n",
    "    :param sep: 表示去掉的部分用什么填充，默认为一个空格\n",
    "    :return: 返回处理后的字符串\n",
    "    example:\n",
    "    s = \"祝你2018000国庆快乐！\"\n",
    "    print(clean_str(s))# 祝你 国庆快乐\n",
    "    print(clean_str(s,sep=\"\"))# 祝你国庆快乐\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^\\u4e00-\\u9fff]\", sep, string)\n",
    "    string = re.sub(r\"\\s{1,}\", sep, string)  # 若有空格，则最多只保留1个宽度\n",
    "    return string.strip()\n",
    "\n",
    "def load_spam():\n",
    "    \"\"\"\n",
    "    载入原始文本\n",
    "    :return: x为一个list，每个元素为一个样本\n",
    "             y为一个list，每个元素为样本对应的标签\n",
    "    \"\"\"\n",
    "    data_spam_dir = DATA_HOME\n",
    "\n",
    "    def load_spam_data(file_path=None):\n",
    "        texts = []\n",
    "        with open(file_path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip('\\n')\n",
    "                texts.append(clean_str(line))\n",
    "        return texts\n",
    "\n",
    "    x_pos = load_spam_data(file_path=os.path.join(data_spam_dir, 'ham_5000.utf8'))\n",
    "    x_neg = load_spam_data(file_path=os.path.join(data_spam_dir, 'spam_5000.utf8'))\n",
    "    y_pos, y_neg = [1] * len(x_pos), [0] * len(x_neg)\n",
    "    x, y = x_pos + x_neg, y_pos + y_neg\n",
    "    return x, y\n",
    "\n",
    "def load_cut_spam():\n",
    "    \"\"\"\n",
    "    :return: ['中信   国际   电子科技 有限公司 推出 新 产品   升职 步步高',\n",
    "             '搜索 文件   看 是否 不 小心 拖 到 某个 地方 了',....]\n",
    "    \"\"\"\n",
    "    x, y = load_spam()\n",
    "    x_cut = []\n",
    "    for text in x:\n",
    "        seg_list = jieba.cut(text, cut_all=False)\n",
    "        tmp = \" \".join(seg_list)\n",
    "        x_cut.append(tmp)\n",
    "    return x_cut, y\n",
    "\n",
    "def get_dataset():\n",
    "    x, y = load_cut_spam()\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c2c81-422f-4aa9-98db-430afa3bdad1",
   "metadata": {},
   "source": [
    "## 2.文本向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e67f1f7-a6dd-42e8-ac13-4e8aa3484b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(x,\n",
    "                  train=False,\n",
    "                  top_k_words=1000,\n",
    "                  MODEL_NAME='count_vec.pkl'):\n",
    "    \"\"\"\n",
    "    数据预处理\n",
    "    :param x: 原始数据\n",
    "    :param train: 训练或测试\n",
    "    :param top_k_words:  取前top_k_words词为词表\n",
    "    :param MODEL_NAME:   模型保存的名称\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        # 仅考虑词频的词袋模型\n",
    "        # count_vec = CountVectorizer(max_features=top_k_words)\n",
    "        # 基于权重的词袋模型\n",
    "        count_vec = TfidfVectorizer(max_features=top_k_words)\n",
    "\n",
    "        count_vec.fit(x)  # 重新训练\n",
    "        # print(len(count_vec.vocabulary_)) # 输出词表长度\n",
    "        save_model(count_vec, MODEL_NAME=MODEL_NAME)\n",
    "    else:\n",
    "        count_vec = load_model(MODEL_NAME=MODEL_NAME)\n",
    "    x = count_vec.transform(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dcdc9a-43ce-4ace-a803-60e398adead0",
   "metadata": {},
   "source": [
    "## 3.保存和载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8d72a9a-a70b-4149-91ed-8b8360505510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, dir='MODEL', MODEL_NAME='model.pkl'):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "    path = os.path.join(dir, MODEL_NAME)\n",
    "    joblib.dump(model, path)\n",
    "    print(f\"模型: {path} 保存成功！\")\n",
    "\n",
    "\n",
    "def load_model(dir='MODEL', MODEL_NAME='model.pkl'):\n",
    "    path = os.path.join(dir, MODEL_NAME)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"{path} 模型不存在，请先训练模型！\")\n",
    "    model = joblib.load(path)\n",
    "    print(f\"载入已有模型: {path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f590df1-4269-4ff2-a2ac-88161d7acf39",
   "metadata": {},
   "source": [
    "## 4.模型训练和预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c640d701-2498-4630-a807-916ce1dc935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train):\n",
    "    X_train = preprocessing(X_train, train=True)\n",
    "    model = KNeighborsClassifier(n_neighbors=3)\n",
    "    model.fit(X_train, y_train)\n",
    "    save_model(model, MODEL_NAME='KNN.pkl')\n",
    "    y_pred = model.predict(X_train)\n",
    "    print(\"模型在训练集上的表现结果：\")\n",
    "    print(classification_report(y_train, y_pred))\n",
    "\n",
    "\n",
    "def predict(X, MODEL_NAME='KNN.pkl'):\n",
    "    X_test = preprocessing(X, train=False)\n",
    "    model = load_model(MODEL_NAME=MODEL_NAME)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e034df64-ba4d-4c2d-9d5e-b36549c187d0",
   "metadata": {},
   "source": [
    "## 5.运行结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a6e1094-5eda-4b66-8164-2037e0715bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型: MODEL/count_vec.pkl 保存成功！\n",
      "模型: MODEL/KNN.pkl 保存成功！\n",
      "模型在训练集上的表现结果：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.80      3537\n",
      "           1       1.00      0.48      0.65      3463\n",
      "\n",
      "    accuracy                           0.74      7000\n",
      "   macro avg       0.83      0.74      0.72      7000\n",
      "weighted avg       0.83      0.74      0.72      7000\n",
      "\n",
      "载入已有模型: MODEL/count_vec.pkl\n",
      "载入已有模型: MODEL/KNN.pkl\n",
      "模型在测试集上的表现结果：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      1.00      0.74      1464\n",
      "           1       0.99      0.33      0.50      1537\n",
      "\n",
      "    accuracy                           0.66      3001\n",
      "   macro avg       0.79      0.66      0.62      3001\n",
      "weighted avg       0.80      0.66      0.62      3001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    X_train, X_test, y_train, y_test = get_dataset()\n",
    "    train(X_train, y_train)\n",
    "    y_pred = predict(X_test)\n",
    "    print(\"模型在测试集上的表现结果：\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
