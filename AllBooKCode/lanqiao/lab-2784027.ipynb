{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2916318-4c7b-47c3-8060-325a8ad5c9aa",
   "metadata": {},
   "source": [
    "# 【实验】第8.5节从零实现ID3决策树算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc8dfd8-5b07-4b56-ba3f-e2b9d9d2b469",
   "metadata": {},
   "source": [
    "## 实验介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1210edcc-5405-4ccb-bd72-59709e3c4081",
   "metadata": {},
   "source": [
    "在本节实验中，我们将详细介绍如何从零实现ID3决策树算法，包括节点定义、决策树构建、剪枝以及垃圾邮件分类的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bdd40e-3100-4dd8-b197-186b7b572f0d",
   "metadata": {},
   "source": [
    "### 知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede075b5-7ecb-4402-afd5-c23e3b40d1e5",
   "metadata": {},
   "source": [
    "- 决策树节点定义\n",
    "- 信息熵计算、信息增益计算、剪枝过程等实现\n",
    "- 文本向量化及垃圾邮件分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcda8a4-a1cb-4db5-a021-b72e1878e832",
   "metadata": {},
   "source": [
    "## 1.定义树节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c784c1a-f9f5-4943-8293-9df182c7f9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<======================>\n",
      "当前节点所有样本的索引([1 2 3 4 5 8])\n",
      "当前节点的样本数量(6)\n",
      "当前节点每个类别的样本数([3 3])\n",
      "当前节点对应的信息增益（比）(0.0)\n",
      "当前节点状态时特征集中剩余特征([0, 1, 3])\n",
      "当前节点状态时划分特征ID(-1)\n",
      "当前节点对应的类别标签为(None)\n",
      "当前节点为根节点对应孩子节点数为(0)\n",
      "当前节点为根节点对应孩子节点损失为(0.0)\n",
      "当前节点对应的孩子为(dict_keys([]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, ):\n",
    "        self.sample_index = None  # 保存当前节点中对应样本在数据集中的索引\n",
    "        self.values = None  # 保存每个类别的数量\n",
    "        self.features = None  # 保存当前节点状态时特征集中剩余特征\n",
    "        self.feature_id = -1  # 保存当前节点对应划分特征的id\n",
    "        self.label = None  # 保存当前节点对应的类别标签（叶子节点才有）\n",
    "        self.n_samples = 0  # 保存当前节点对应的样本数量\n",
    "        self.children = {}  # 保存当前节点对应的孩子节点\n",
    "        self.criterion_value = 0.\n",
    "        self.n_leaf = 0  # 以当前节点为根节点时其叶子节点的个数\n",
    "        self.leaf_costs = 0.  # 以当前节点为根节点时其所有叶子节点的损失和\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        打印节点信息\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return f\"<======================>\\n\" \\\n",
    "               f\"当前节点所有样本的索引({self.sample_index})\\n\" \\\n",
    "               f\"当前节点的样本数量({self.n_samples})\\n\" \\\n",
    "               f\"当前节点每个类别的样本数({self.values})\\n\" \\\n",
    "               f\"当前节点对应的信息增益（比）({round(self.criterion_value, 3)})\\n\" \\\n",
    "               f\"当前节点状态时特征集中剩余特征({self.features})\\n\" \\\n",
    "               f\"当前节点状态时划分特征ID({self.feature_id})\\n\" \\\n",
    "               f\"当前节点对应的类别标签为({self.label})\\n\" \\\n",
    "               f\"当前节点为根节点对应孩子节点数为({self.n_leaf})\\n\" \\\n",
    "               f\"当前节点为根节点对应孩子节点损失为({self.leaf_costs})\\n\" \\\n",
    "               f\"当前节点对应的孩子为({self.children.keys()})\"\n",
    "\n",
    "node = Node()\n",
    "node.sample_index = np.array([1,2,3,4,5,8])  # 当前节点所有样本的索引\n",
    "labels = np.array([0,0,0,1,1,1])  # 当前节点所有样本对应的标签\n",
    "node.n_samples = len(labels)  # 当前节点的样本数量\n",
    "node.values = np.bincount(labels, minlength=2)  # 当前节点每个类别的样本数\n",
    "node.features = [0,1,3]  # 当前节点状态时特征集中剩余特征\n",
    "\n",
    "print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009bcbdd-224b-4d7f-9cfe-20e02b2173f2",
   "metadata": {},
   "source": [
    "## 2.从零实现ID3决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38317c58-278c-480a-a16e-c2c36e541284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, min_samples_split=2,\n",
    "                 criterion='id3',\n",
    "                 epsilon=1e-5,\n",
    "                 alpha=0.):\n",
    "        self.root = None\n",
    "        self.min_samples_split = min_samples_split  # 用来控制是否停止分裂\n",
    "        self.epsilon = epsilon  # 用来控制是否停止分裂\n",
    "        self.criterion = criterion  # 划分标注，ID3还是C4.5\n",
    "        self.alpha = alpha\n",
    "        # criterion = \"id3\" 表示使用ID3进行决策树构建\n",
    "        # criterion = \"c45\" 表示使用C4.5进行决策树\n",
    "\n",
    "    def _compute_entropy(self, y_class):\n",
    "        \"\"\"\n",
    "        计算信息熵（主要用于ID3中计算样本的信息熵，以及C4.5中特征的信息熵）\n",
    "        :param y_class:  np.array   [n,]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        y_unique = np.unique(y_class)\n",
    "        if y_unique.shape[0] == 1:  # 只有一个类别\n",
    "            return 0.  # 熵为0\n",
    "        ety = 0.\n",
    "        for i in range(len(y_unique)):  # 取每个类别\n",
    "            p = np.sum(y_class == y_unique[i]) / len(y_class)\n",
    "            ety += p * np.log2(p)\n",
    "        return -ety\n",
    "\n",
    "    def _compute_condition_entropy(self, X_feature, y_class):\n",
    "        \"\"\"\n",
    "        计算条件熵\n",
    "        :param X_feature: shape: [n,]\n",
    "        :param y_class: shape: [n,]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        f_unique = np.unique(X_feature)\n",
    "        result = 0.\n",
    "        for i in range(len(f_unique)):  # 取每个特征类别\n",
    "            index_x = (X_feature == f_unique[i])  # 取当前特征类别对应的索引\n",
    "            p = np.sum(index_x) / len(y_class)  # 取索引对应的标签\n",
    "            ety = self._compute_entropy(y_class[index_x])  # 计算标签对应的信息熵\n",
    "            result += p * ety  # 计算条件熵\n",
    "        return result\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        输入的数据集X特征必须为categorical类型\n",
    "        :param X: shape: [n_samples, n_features]\n",
    "        :param y: shape: [n_samples,]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._y = np.array(y).reshape(-1)\n",
    "        self.n_classes = len(np.bincount(y))  # 得到当前数据集的类别数量\n",
    "        feature_ids = [i for i in range(X.shape[1])]  # 得到特征的序号\n",
    "        self._X = np.hstack(([X, np.arange(len(X)).reshape(-1, 1)]))\n",
    "        # 将训练集中每个样本的序号加入到X的最后一列\n",
    "        self._build_tree(self._X, feature_ids)  # 递归构建决策树\n",
    "        self._pruning_leaf()\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_label(labels):\n",
    "        \"\"\"\n",
    "        根据给定标签，返回出现次数最多的类别\n",
    "\n",
    "        :param labels: ['1', '1', '1', '0', '0', '0', '0', '2']\n",
    "        :return: '0'\n",
    "        \"\"\"\n",
    "        r = {}\n",
    "        for i in range(len(labels)):\n",
    "            r[labels[i]] = r.setdefault(labels[i], 0) + 1\n",
    "        return sorted(r.items(), key=lambda x: x[1])[-1][0]\n",
    "\n",
    "    def _split_criterion(self, ety, X_feature, y_class):\n",
    "        c_ety = self._compute_condition_entropy(X_feature, y_class)  # 计算每个特征下的条件熵\n",
    "        logging.debug(\"\")\n",
    "        logging.debug(f\"【当前节点下特征对应的条件熵为 {c_ety}\")\n",
    "        info_gains = ety - c_ety  # 计算信息增益\n",
    "        if self.criterion == \"id3\":\n",
    "            return info_gains\n",
    "        elif self.criterion == \"c45\":\n",
    "            f_ety = self._compute_entropy(X_feature)\n",
    "            logging.debug(f\"当前节点下特征对应的信息熵为 {f_ety}\")\n",
    "            return info_gains / f_ety\n",
    "        else:\n",
    "            raise ValueError(f\"划分标准 self.criterion = {self.criterion}只能是 id3 和 c45 其中之一！\")\n",
    "\n",
    "    def _build_tree(self, data, f_ids):\n",
    "        \"\"\"\n",
    "        :param x_ids: np.array() [n,] 样本索引，用于在节点中保存每个样本的索引，以及根据索引取到对应样本\n",
    "        :param f_ids: list 特征序号，用于在当前节点中保存特征集中还剩余哪些特征\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x_ids = np.array(data[:, -1], dtype=int).reshape(-1)\n",
    "        node = Node()\n",
    "        node.sample_index = x_ids  # 当前节点所有样本的索引\n",
    "        labels = self._y[x_ids]  # 当前节点所有样本对应的标签\n",
    "        node.n_samples = len(labels)  # 当前节点的样本数量\n",
    "        node.values = np.bincount(labels, minlength=self.n_classes)  # 当前节点每个类别的样本数\n",
    "        node.features = f_ids  # 当前节点状态时特征集中剩余特征\n",
    "        logging.debug(f\"当前节点所有样本的索引 {node.sample_index}\")\n",
    "        logging.debug(f\"当前节点的样本数量 {node.n_samples}\")\n",
    "        logging.debug(f\"当前节点每个类别的样本数 {node.values}\")\n",
    "        logging.debug(f\"当前节点状态时特征集中剩余特征 {node.features}\")\n",
    "        if self.root is None:\n",
    "            self.root = node\n",
    "\n",
    "        y_unique = np.unique(labels)  # 当前节点一种存在的类别数量\n",
    "        if y_unique.shape[0] == 1 or len(f_ids) < 1 \\\n",
    "                or node.n_samples <= self.min_samples_split:  # 只有一个类别或特征集为空或样本数量少于min_samples_split\n",
    "            node.label = self._get_label(labels)  # 根据多数原则确定当前节点对应的类别\n",
    "            logging.debug(f\"**当前节点满足停止条件，不再继续往下进行分裂，节点对应的类别为 {node.label}\")\n",
    "            return node\n",
    "        ety = self._compute_entropy(labels)  # 计算当前节点所有样本对应的信息熵\n",
    "        node.criterion_value = ety\n",
    "        logging.debug(f\"当前节点中的样本信息熵为 {ety}\")\n",
    "        max_criterion = 0\n",
    "        best_feature_id = -1\n",
    "        for id in f_ids:  # 根据划分标准（如信息增益）选择最佳特征\n",
    "            criterion = self._split_criterion(ety, data[:, id], labels)\n",
    "            logging.debug(f\"当前节点第{id}个特征在标准{self.criterion}下对应的划分指标为{criterion}】\")\n",
    "            if criterion > max_criterion:  # 遍历选择最大指标（信息增益或信息增益比）\n",
    "                max_criterion = criterion\n",
    "                best_feature_id = id\n",
    "        if max_criterion < self.epsilon:  # 最大指标小于设定阈值\n",
    "            node.label = self._get_label(labels)  # 根据多数原则确定当前节点对应的类别\n",
    "            logging.debug(f\"**当前节点满足停止条件(最大指标小于设定阈值)，不再继续往下进行分裂，节点对应的类别为 {node.label}\")\n",
    "            return node\n",
    "        node.feature_id = best_feature_id\n",
    "        logging.debug(f\"此时选择第{best_feature_id}个特征进行样本划分\")\n",
    "        feature_values = np.unique(data[:, best_feature_id])  # 划分特征的取值情况\n",
    "        logging.debug(f\"此时第{best_feature_id}个特征的取值为{feature_values}\")\n",
    "        candidate_ids = copy.copy(f_ids)\n",
    "        candidate_ids.remove(best_feature_id)  # 当前节点划分后的剩余特征集\n",
    "\n",
    "        for f in feature_values:  # 依次遍历每个取值情况\n",
    "            logging.debug(\"\")\n",
    "            logging.debug(\"========>\")\n",
    "            logging.debug(f\"往下递归遍历最佳特征（第<{best_feature_id}>个）的取值 {f} 对应的子树\")\n",
    "            c = data[:, best_feature_id] == f  # 根据当前特征维度的取值，来判断对应的样本\n",
    "            index = np.array([i for i in range(len(c)) if c[i] == True])  # 获取对应的索引\n",
    "            node.children[f] = self._build_tree(data[index], candidate_ids)\n",
    "        return node\n",
    "\n",
    "    def level_order(self, return_node=False):\n",
    "        \"\"\"\n",
    "        层次遍历\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        logging.debug(\"正在进行层次遍历……\")\n",
    "        root = self.root\n",
    "        if not root:\n",
    "            return []\n",
    "        queue = [root]\n",
    "        res = []\n",
    "        while queue:\n",
    "            tmp = []\n",
    "            for i in range(len(queue)):\n",
    "                node = queue.pop(0)\n",
    "                tmp.append(node)\n",
    "                for k, v in node.children.items():\n",
    "                    queue.append(v)\n",
    "            res.append(tmp)\n",
    "        if return_node:\n",
    "            logging.debug(f\"并返回层次遍历后的结果\\n\")\n",
    "            return res  # 按层次遍历的顺序返回各层节点的地址\n",
    "            # [[root], [level2 node1, level2_node2], [level3,...] [level4,...],...[],]\n",
    "        logging.debug(\"\\n层次遍历结果为：\")\n",
    "        for i, r in enumerate(res):\n",
    "            logging.debug(f\"第{i + 1}层的节点为：\")\n",
    "            for node in r:\n",
    "                logging.debug(node.sample_index)\n",
    "            logging.debug(\"\\n\")\n",
    "\n",
    "    def _predict_one_sample(self, x):\n",
    "        \"\"\"\n",
    "        预测单一样本\n",
    "        :param x: [n_features,]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        current_node = self.root\n",
    "        while True:\n",
    "            current_feature_id = current_node.feature_id\n",
    "            current_feature = x[current_feature_id]\n",
    "            if len(current_node.children) < 1 or \\\n",
    "                    current_feature not in current_node.children:\n",
    "                # ① 当前节点为叶子节点，或者由于数据集不充分当前节点的孩子节点不存在下一个划分节点的某一个取值\n",
    "                # ② 例如根据测试数据集load_simple_data（）构造得到的id3树，对于特征['0','1','D']来说，\n",
    "                # ③ 在遍历最后一个特征维度时，取值'D'就不存在于孩子节点中\n",
    "                return current_node.values\n",
    "            current_node = current_node.children[current_feature]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: shape [n_samples,n_features]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for x in X:\n",
    "            results.append(self._predict_one_sample(x))\n",
    "        results = np.array(results)\n",
    "        logging.debug(f\"原始预测结果为:\\n{results}\")\n",
    "        y_pred = np.argmax(results, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "    def _pruning_leaf(self):\n",
    "        level_order_nodes = self.level_order(return_node=True)\n",
    "        # 获取得到层次遍历的所有结果\n",
    "        logging.debug(f\"正在进行剪枝操作……\")\n",
    "        for i in range(len(level_order_nodes) - 1, -1, -1):\n",
    "            # 从下往上依次遍历每一层节点\n",
    "            current_level_nodes = level_order_nodes[i]  # 取第i层的所有节点\n",
    "            for j in range(len(current_level_nodes)):\n",
    "                current_node = current_level_nodes[j]  # 取第i层的第j个节点\n",
    "                if len(current_node.children) == 0:  # 当前节点为叶子节点时\n",
    "                    current_node.n_leaf = 1  # 令其叶节点个数为1\n",
    "                else:\n",
    "                    for _, leaf_node in current_node.children.items():\n",
    "                        current_node.n_leaf += leaf_node.n_leaf  # 统计以当前节点为根节点时的叶子节点数量\n",
    "                if self._is_pruning_leaf(current_node):\n",
    "                    current_node.children = {}\n",
    "                    current_node.n_leaf = 1\n",
    "\n",
    "    def _is_pruning_leaf(self, node):\n",
    "        \"\"\"\n",
    "        判断是否对当前节点进行剪枝\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        def _compute_cost_in_leaf(labels):\n",
    "            \"\"\"\n",
    "            计算节点的损失  c = -\\sum_{k=1}^KN_{tk}\\log{\\frac{N_{tk}}{N_t}}\n",
    "            :param labels:\n",
    "            :return:\n",
    "            e.g. y_labels = np.array([1, 1, 1, 0])\n",
    "            _compute_cost_in_leaf(y_labels)   3.24511\n",
    "            \"\"\"\n",
    "            y_count = np.bincount(labels)\n",
    "            n_samples = len(labels)\n",
    "            cost = 0\n",
    "            for i in range(len(y_count)):\n",
    "                if y_count[i] == 0:\n",
    "                    continue\n",
    "                cost += y_count[i] * np.log2(y_count[i] / n_samples)\n",
    "            return -cost\n",
    "\n",
    "        if len(node.children) < 1:  # 当前节点为叶子节点时，计算叶子节点对应的损失\n",
    "            node.leaf_costs = _compute_cost_in_leaf(self._y[node.sample_index])\n",
    "            return False\n",
    "        parent_cost = _compute_cost_in_leaf(self._y[node.sample_index])  # 剪枝后的损失\n",
    "        for (_, leaf_node) in node.children.items():  # 剪枝前累加所有叶子节点的损失\n",
    "            node.leaf_costs += leaf_node.leaf_costs\n",
    "        logging.debug(f\"当前节点的损失为：{parent_cost} + {self.alpha}\")\n",
    "        logging.debug(f\"当前节点的孩子节点损失和为：{node.leaf_costs} + {self.alpha} * {node.n_leaf}\")\n",
    "        if node.leaf_costs + self.alpha * node.n_leaf > parent_cost + self.alpha:\n",
    "            #  当剪枝前的损失  >  剪枝后的损失， 则表示当前节点可以进行剪枝（减掉其所有孩子）\n",
    "            return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0ccc0-dbb3-46ec-94c4-ab726169f906",
   "metadata": {},
   "source": [
    "## 3.构建计算示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "382511d8-9900-46cb-9e37-5094b9369736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_simple_data():\n",
    "    x = np.array([['0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '1', '1', '1'],\n",
    "                  ['1', '1', '1', '0', '1', '0', '0', '0', '0', '0', '1', '1', '1', '0', '0'],\n",
    "                  ['T', 'S', 'S', 'T', 'T', 'T', 'D', 'T', 'T', 'D', 'D', 'T', 'T', 'S', 'S']]).transpose()\n",
    "    y = np.array([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58534b41-af1a-4a5e-8596-73bee170ebd6",
   "metadata": {},
   "source": [
    "## 4.测试计算信息熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e56307b-aff1-486c-a347-e8745a5897c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "信息熵0.9182958340544896\n",
      "条件熵为0.7496741665224355\n",
      "条件熵为0.8094472966717527\n",
      "条件熵为0.909031468226648\n"
     ]
    }
   ],
   "source": [
    "def test_compute():\n",
    "    x, y = load_simple_data()\n",
    "    dt = DecisionTree()\n",
    "    ety = dt._compute_entropy(y)\n",
    "    print(f\"信息熵{ety}\")\n",
    "    for i in range(x.shape[1]):\n",
    "        print(f\"条件熵为{dt._compute_condition_entropy(x[:, i], y)}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    formatter = '[%(asctime)s] - %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(level=logging.DEBUG,  # 如果需要查看简略信息可将该参数改为logging.INFO\n",
    "                        format=formatter,  # 关于Logging模块的详细使用可参加文章 https://mp.weixin.qq.com/s/cvO6hCiHMJqC4-4AuUlydw\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                        handlers=[logging.StreamHandler(sys.stdout)])\n",
    "    test_compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db6f8d-c0bc-487d-928f-cd0b3105d04d",
   "metadata": {},
   "source": [
    "## 5.测试决策树的构建及预测新样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80e4ea64-556a-41a0-a5b8-6872d601b925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-14 14:42:22] - DEBUG: 当前节点所有样本的索引 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点的样本数量 15\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点每个类别的样本数 [ 5 10]\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点状态时特征集中剩余特征 [0, 1, 2]\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点中的样本信息熵为 0.9182958340544896\n",
      "[2024-06-14 14:42:22] - DEBUG: \n",
      "[2024-06-14 14:42:22] - DEBUG: 【当前节点下特征对应的条件熵为 0.7496741665224355\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点第0个特征在标准id3下对应的划分指标为0.1686216675320541】\n",
      "[2024-06-14 14:42:22] - DEBUG: \n",
      "[2024-06-14 14:42:22] - DEBUG: 【当前节点下特征对应的条件熵为 0.8094472966717527\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点第1个特征在标准id3下对应的划分指标为0.10884853738273681】\n",
      "[2024-06-14 14:42:22] - DEBUG: \n",
      "[2024-06-14 14:42:22] - DEBUG: 【当前节点下特征对应的条件熵为 0.909031468226648\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点第2个特征在标准id3下对应的划分指标为0.009264365827841514】\n",
      "[2024-06-14 14:42:22] - DEBUG: 此时选择第0个特征进行样本划分\n",
      "[2024-06-14 14:42:22] - DEBUG: 此时第0个特征的取值为['0' '1']\n",
      "[2024-06-14 14:42:22] - DEBUG: \n",
      "[2024-06-14 14:42:22] - DEBUG: ========>\n",
      "[2024-06-14 14:42:22] - DEBUG: 往下递归遍历最佳特征（第<0>个）的取值 0 对应的子树\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点所有样本的索引 [0 1 2 3 4 5 6]\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点的样本数量 7\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点每个类别的样本数 [4 3]\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点状态时特征集中剩余特征 [1, 2]\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点中的样本信息熵为 0.9852281360342515\n",
      "[2024-06-14 14:42:22] - DEBUG: \n",
      "[2024-06-14 14:42:22] - DEBUG: 【当前节点下特征对应的条件熵为 0.46358749969093305\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点第1个特征在标准id3下对应的划分指标为0.5216406363433185】\n",
      "[2024-06-14 14:42:22] - DEBUG: \n",
      "[2024-06-14 14:42:22] - DEBUG: 【当前节点下特征对应的条件熵为 0.46358749969093305\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点第2个特征在标准id3下对应的划分指标为0.5216406363433185】\n",
      "[2024-06-14 14:42:22] - DEBUG: 此时选择第1个特征进行样本划分\n",
      "[2024-06-14 14:42:22] - DEBUG: 此时第1个特征的取值为['0' '1']\n",
      "[2024-06-14 14:42:22] - DEBUG: \n",
      "[2024-06-14 14:42:22] - DEBUG: ========>\n",
      "[2024-06-14 14:42:22] - DEBUG: 往下递归遍历最佳特征（第<1>个）的取值 0 对应的子树\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点所有样本的索引 [3 5 6]\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点的样本数量 3\n",
      "[2024-06-14 14:42:22] - DEBUG: 当前节点每个类别的样本数 [3 0]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点状态时特征集中剩余特征 [2]\n",
      "[2024-06-14 14:42:23] - DEBUG: **当前节点满足停止条件，不再继续往下进行分裂，节点对应的类别为 0\n",
      "[2024-06-14 14:42:23] - DEBUG: \n",
      "[2024-06-14 14:42:23] - DEBUG: ========>\n",
      "[2024-06-14 14:42:23] - DEBUG: 往下递归遍历最佳特征（第<1>个）的取值 1 对应的子树\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点所有样本的索引 [0 1 2 4]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点的样本数量 4\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点每个类别的样本数 [1 3]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点状态时特征集中剩余特征 [2]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点中的样本信息熵为 0.8112781244591328\n",
      "[2024-06-14 14:42:23] - DEBUG: \n",
      "[2024-06-14 14:42:23] - DEBUG: 【当前节点下特征对应的条件熵为 0.5\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点第2个特征在标准id3下对应的划分指标为0.31127812445913283】\n",
      "[2024-06-14 14:42:23] - DEBUG: 此时选择第2个特征进行样本划分\n",
      "[2024-06-14 14:42:23] - DEBUG: 此时第2个特征的取值为['S' 'T']\n",
      "[2024-06-14 14:42:23] - DEBUG: \n",
      "[2024-06-14 14:42:23] - DEBUG: ========>\n",
      "[2024-06-14 14:42:23] - DEBUG: 往下递归遍历最佳特征（第<2>个）的取值 S 对应的子树\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点所有样本的索引 [1 2]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点的样本数量 2\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点每个类别的样本数 [0 2]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点状态时特征集中剩余特征 []\n",
      "[2024-06-14 14:42:23] - DEBUG: **当前节点满足停止条件，不再继续往下进行分裂，节点对应的类别为 1\n",
      "[2024-06-14 14:42:23] - DEBUG: \n",
      "[2024-06-14 14:42:23] - DEBUG: ========>\n",
      "[2024-06-14 14:42:23] - DEBUG: 往下递归遍历最佳特征（第<2>个）的取值 T 对应的子树\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点所有样本的索引 [0 4]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点的样本数量 2\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点每个类别的样本数 [1 1]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点状态时特征集中剩余特征 []\n",
      "[2024-06-14 14:42:23] - DEBUG: **当前节点满足停止条件，不再继续往下进行分裂，节点对应的类别为 0\n",
      "[2024-06-14 14:42:23] - DEBUG: \n",
      "[2024-06-14 14:42:23] - DEBUG: ========>\n",
      "[2024-06-14 14:42:23] - DEBUG: 往下递归遍历最佳特征（第<0>个）的取值 1 对应的子树\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点所有样本的索引 [ 7  8  9 10 11 12 13 14]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点的样本数量 8\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点每个类别的样本数 [1 7]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点状态时特征集中剩余特征 [1, 2]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点中的样本信息熵为 0.5435644431995964\n",
      "[2024-06-14 14:42:23] - DEBUG: \n",
      "[2024-06-14 14:42:23] - DEBUG: 【当前节点下特征对应的条件熵为 0.4512050593046014\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点第1个特征在标准id3下对应的划分指标为0.09235938389499498】\n",
      "[2024-06-14 14:42:23] - DEBUG: \n",
      "[2024-06-14 14:42:23] - DEBUG: 【当前节点下特征对应的条件熵为 0.25\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点第2个特征在标准id3下对应的划分指标为0.2935644431995964】\n",
      "[2024-06-14 14:42:23] - DEBUG: 此时选择第2个特征进行样本划分\n",
      "[2024-06-14 14:42:23] - DEBUG: 此时第2个特征的取值为['D' 'S' 'T']\n",
      "[2024-06-14 14:42:23] - DEBUG: \n",
      "[2024-06-14 14:42:23] - DEBUG: ========>\n",
      "[2024-06-14 14:42:23] - DEBUG: 往下递归遍历最佳特征（第<2>个）的取值 D 对应的子树\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点所有样本的索引 [ 9 10]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点的样本数量 2\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点每个类别的样本数 [0 2]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点状态时特征集中剩余特征 [1]\n",
      "[2024-06-14 14:42:23] - DEBUG: **当前节点满足停止条件，不再继续往下进行分裂，节点对应的类别为 1\n",
      "[2024-06-14 14:42:23] - DEBUG: \n",
      "[2024-06-14 14:42:23] - DEBUG: ========>\n",
      "[2024-06-14 14:42:23] - DEBUG: 往下递归遍历最佳特征（第<2>个）的取值 S 对应的子树\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点所有样本的索引 [13 14]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点的样本数量 2\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点每个类别的样本数 [1 1]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点状态时特征集中剩余特征 [1]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点中的样本信息熵为 1.0\n",
      "[2024-06-14 14:42:23] - DEBUG: \n",
      "[2024-06-14 14:42:23] - DEBUG: 【当前节点下特征对应的条件熵为 1.0\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点第1个特征在标准id3下对应的划分指标为0.0】\n",
      "[2024-06-14 14:42:23] - DEBUG: **当前节点满足停止条件(最大指标小于设定阈值)，不再继续往下进行分裂，节点对应的类别为 1\n",
      "[2024-06-14 14:42:23] - DEBUG: \n",
      "[2024-06-14 14:42:23] - DEBUG: ========>\n",
      "[2024-06-14 14:42:23] - DEBUG: 往下递归遍历最佳特征（第<2>个）的取值 T 对应的子树\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点所有样本的索引 [ 7  8 11 12]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点的样本数量 4\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点每个类别的样本数 [0 4]\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点状态时特征集中剩余特征 [1]\n",
      "[2024-06-14 14:42:23] - DEBUG: **当前节点满足停止条件，不再继续往下进行分裂，节点对应的类别为 1\n",
      "[2024-06-14 14:42:23] - DEBUG: 正在进行层次遍历……\n",
      "[2024-06-14 14:42:23] - DEBUG: 并返回层次遍历后的结果\n",
      "\n",
      "[2024-06-14 14:42:23] - DEBUG: 正在进行剪枝操作……\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点的损失为：3.2451124978365313 + 0.0\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点的孩子节点损失和为：2.0 + 0.0 * 2\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点的损失为：6.896596952239761 + 0.0\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点的孩子节点损失和为：2.0 + 0.0 * 3\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点的损失为：4.348515545596771 + 0.0\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点的孩子节点损失和为：2.0 + 0.0 * 3\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点的损失为：13.774437510817343 + 0.0\n",
      "[2024-06-14 14:42:23] - DEBUG: 当前节点的孩子节点损失和为：4.0 + 0.0 * 6\n",
      "[2024-06-14 14:42:23] - DEBUG: 正在进行层次遍历……\n",
      "[2024-06-14 14:42:23] - DEBUG: \n",
      "层次遍历结果为：\n",
      "[2024-06-14 14:42:23] - DEBUG: 第1层的节点为：\n",
      "[2024-06-14 14:42:23] - DEBUG: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "[2024-06-14 14:42:23] - DEBUG: \n",
      "\n",
      "[2024-06-14 14:42:23] - DEBUG: 第2层的节点为：\n",
      "[2024-06-14 14:42:23] - DEBUG: [0 1 2 3 4 5 6]\n",
      "[2024-06-14 14:42:23] - DEBUG: [ 7  8  9 10 11 12 13 14]\n",
      "[2024-06-14 14:42:23] - DEBUG: \n",
      "\n",
      "[2024-06-14 14:42:23] - DEBUG: 第3层的节点为：\n",
      "[2024-06-14 14:42:23] - DEBUG: [3 5 6]\n",
      "[2024-06-14 14:42:23] - DEBUG: [0 1 2 4]\n",
      "[2024-06-14 14:42:23] - DEBUG: [ 9 10]\n",
      "[2024-06-14 14:42:23] - DEBUG: [13 14]\n",
      "[2024-06-14 14:42:23] - DEBUG: [ 7  8 11 12]\n",
      "[2024-06-14 14:42:23] - DEBUG: \n",
      "\n",
      "[2024-06-14 14:42:23] - DEBUG: 第4层的节点为：\n",
      "[2024-06-14 14:42:23] - DEBUG: [1 2]\n",
      "[2024-06-14 14:42:23] - DEBUG: [0 4]\n",
      "[2024-06-14 14:42:23] - DEBUG: \n",
      "\n",
      "[2024-06-14 14:42:23] - DEBUG: 原始预测结果为:\n",
      "[[3 0]\n",
      " [0 2]\n",
      " [1 3]\n",
      " [1 1]]\n",
      "[2024-06-14 14:42:23] - INFO: DecisionTree 预测结果为：[0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "def test_decision_tree():\n",
    "    x, y = load_simple_data()\n",
    "    dt = DecisionTree(min_samples_split=1)\n",
    "    dt.fit(x, y)\n",
    "    dt.level_order()\n",
    "    y_pred = dt.predict(np.array([['0', '0', 'T'],\n",
    "                                  ['0', '1', 'S'],\n",
    "                                  ['0', '1', 'D'],\n",
    "                                  ['0', '1', 'T']]))\n",
    "    logging.info(f\"DecisionTree 预测结果为：{y_pred}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    formatter = '[%(asctime)s] - %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(level=logging.DEBUG,  # 如果需要查看简略信息可将该参数改为logging.INFO\n",
    "                        format=formatter,  # 关于Logging模块的详细使用可参加文章 https://mp.weixin.qq.com/s/cvO6hCiHMJqC4-4AuUlydw\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                        handlers=[logging.StreamHandler(sys.stdout)])\n",
    "    test_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99234cbd-9a38-49ca-96b6-34048c238216",
   "metadata": {},
   "source": [
    "## 6. 文本向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1779d3c2-a393-42ca-9a9f-3ac2939eb03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VectWithoutFrequency(object):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, top_k_words=500):\n",
    "        self.top_k_words = top_k_words\n",
    "\n",
    "    def _get_vocab(self, raw_documents):\n",
    "\n",
    "        c = Counter()\n",
    "        for sample in raw_documents:\n",
    "            words_list = sample.split()\n",
    "            for x in words_list:\n",
    "                if len(x) > 1 and x != '\\r\\n':\n",
    "                    c[x] += 1\n",
    "        # ---------词频统计构造词表------------------\n",
    "        vocab = []\n",
    "        for (k, v) in c.most_common(self.top_k_words):  # 输出词频最高的前top_k_words个词\n",
    "            vocab.append(k)\n",
    "        return vocab\n",
    "\n",
    "    def fit_transform(self, raw_documents):\n",
    "        \"\"\"\n",
    "        拟合\n",
    "        :param raw_documents: 原始样本，list， 每个元素为分词后的样本\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.fit(raw_documents)\n",
    "        x = self.transform(raw_documents)\n",
    "        return x\n",
    "\n",
    "    def transform(self, raw_documents):\n",
    "        \"\"\"\n",
    "        :param raw_documents:\n",
    "        :return:\n",
    "        e.g.\n",
    "        s = ['文本 分词 工具 可 用于 对 文本 进行 分词 处理', '常见 的 用于 处理 文本 的 分词 处理 工具 有 很多']\n",
    "        vect = VectWithoutFrequency()\n",
    "          x = vect.fit_transform(s)\n",
    "          vect.vocab: ['文本', '分词', '处理', '工具', '用于', '进行', '常见', '很多']\n",
    "        x:\n",
    "          [[1, 1, 1, 1, 1, 1, 0, 0],\n",
    "           [1, 1, 1, 1, 1, 0, 1, 1]]\n",
    "        \"\"\"\n",
    "        x_vec = []\n",
    "        for item in raw_documents:\n",
    "            tmp = [0] * len(self.vocabulary)\n",
    "            for i, w in enumerate(self.vocabulary):\n",
    "                if w in item:\n",
    "                    tmp[i] = 1\n",
    "            x_vec.append(tmp)\n",
    "        return np.array(x_vec)\n",
    "\n",
    "    def fit(self, raw_documents):\n",
    "        self.vocabulary = self._get_vocab(raw_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ece6cc-a148-4f86-a168-7ba49441ccaa",
   "metadata": {},
   "source": [
    "## 7. 载入垃圾邮件数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41d076b8-ab28-4f13-be49-b5b56848ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba \n",
    "\n",
    "\n",
    "def clean_str(string, sep=\" \"):\n",
    "    \"\"\"\n",
    "    该函数的作用是去掉一个字符串中的所有非中文字符\n",
    "    :param string: 输入必须是字符串类型\n",
    "    :param sep: 表示去掉的部分用什么填充，默认为一个空格\n",
    "    :return: 返回处理后的字符串\n",
    "    example:\n",
    "    s = \"祝你2018000国庆快乐！\"\n",
    "    print(clean_str(s))# 祝你 国庆快乐\n",
    "    print(clean_str(s,sep=\"\"))# 祝你国庆快乐\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^\\u4e00-\\u9fff]\", sep, string)\n",
    "    string = re.sub(r\"\\s{1,}\", sep, string)  # 若有空格，则最多只保留1个宽度\n",
    "    return string.strip()\n",
    "    \n",
    "def load_data():\n",
    "    x, y = load_cut_spam()\n",
    "    x_train, x_test, y_train, y_test \\\n",
    "        = train_test_split(x, y, test_size=0.3, random_state=2020)\n",
    "    vect = VectWithoutFrequency(top_k_words=1000)\n",
    "    x_train = vect.fit_transform(x_train)\n",
    "    x_test = vect.transform(x_test)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def load_spam():\n",
    "    \"\"\"\n",
    "    载入原始文本\n",
    "    :return: x为一个list，每个元素为一个样本\n",
    "             y为一个list，每个元素为样本对应的标签\n",
    "    \"\"\"\n",
    "    data_spam_dir = os.path.join(os.path.dirname(os.path.dirname('./')), 'data')\n",
    "\n",
    "    def load_spam_data(file_path=None):\n",
    "        texts = []\n",
    "        with open(file_path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip('\\n')\n",
    "                texts.append(clean_str(line))\n",
    "        return texts\n",
    "\n",
    "    x_pos = load_spam_data(file_path=os.path.join(data_spam_dir, 'ham_5000.utf8'))\n",
    "    x_neg = load_spam_data(file_path=os.path.join(data_spam_dir, 'spam_5000.utf8'))\n",
    "    y_pos, y_neg = [1] * len(x_pos), [0] * len(x_neg)\n",
    "    x, y = x_pos + x_neg, y_pos + y_neg\n",
    "    return x, y\n",
    "    \n",
    "def load_cut_spam():\n",
    "    \"\"\"\n",
    "    :return: ['中信   国际   电子科技 有限公司 推出 新 产品   升职 步步高',\n",
    "             '搜索 文件   看 是否 不 小心 拖 到 某个 地方 了',....]\n",
    "    \"\"\"\n",
    "    x, y = load_spam()\n",
    "    x_cut = []\n",
    "    for text in x:\n",
    "        seg_list = jieba.cut(text, cut_all=False)\n",
    "        tmp = \" \".join(seg_list)\n",
    "        x_cut.append(tmp)\n",
    "    return x_cut, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ab6732-0a02-4601-98ab-2859dd6764b2",
   "metadata": {},
   "source": [
    "## 8. 垃圾邮件分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fbb983b-3619-4888-8ecd-8aefd78d21b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-14 14:51:05] - INFO: DecisionTree 准确率：0.9770076641119627\n",
      "[2024-06-14 14:51:05] - INFO: DecisionTreeClassifier 准确率：0.9776741086304566\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test_spam_classification():\n",
    "    x_train, x_test, y_train, y_test = load_data()\n",
    "    dt = DecisionTree(criterion=\"id3\")\n",
    "    dt.fit(x_train, y_train)\n",
    "    y_pred = dt.predict(x_test)\n",
    "    logging.info(f\"DecisionTree 准确率：{accuracy_score(y_test, y_pred)}\")\n",
    "    # logging.info(f\"DecisionTree 预测结果为：{y_pred}\")\n",
    "    model = DecisionTreeClassifier(criterion='entropy')\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    logging.info(f\"DecisionTreeClassifier 准确率：{accuracy_score(y_test, y_pred)}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    formatter = '[%(asctime)s] - %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(level=logging.INFO,  # 如果需要查看简略信息可将该参数改为logging.INFO\n",
    "                        format=formatter,  # 关于Logging模块的详细使用可参加文章 https://mp.weixin.qq.com/s/cvO6hCiHMJqC4-4AuUlydw\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                        handlers=[logging.StreamHandler(sys.stdout)])\n",
    "    test_spam_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05886a07-de7a-4dc6-91e1-2c8c8404384f",
   "metadata": {},
   "source": [
    "## 9.剪枝测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36f05d04-c4f1-4e2b-b5be-c7003c71f7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-14 14:56:14] - INFO: 剪枝前的预测类别：[1 0]\n",
      "[2024-06-14 14:56:14] - INFO: 剪枝后的预测类别：[1 1]\n"
     ]
    }
   ],
   "source": [
    "def test_decision_tree_pruning():\n",
    "    x, y = load_simple_data()\n",
    "    dt = DecisionTree(criterion='id3', alpha=0.)\n",
    "    dt.fit(x, y)\n",
    "    x_test = np.array([['0', '1', 'S'],\n",
    "                       ['0', '1', 'T']])\n",
    "    logging.debug(f\"剪枝前的层次遍历结果\")\n",
    "    # dt.level_order()\n",
    "    logging.info(f\"剪枝前的预测类别：{dt.predict(x_test)}\")\n",
    "\n",
    "    dt = DecisionTree(criterion='id3', alpha=1.25)\n",
    "    dt.fit(x, y)\n",
    "    logging.debug(f\"剪枝后的层次遍历结果\")\n",
    "    # dt.level_order()\n",
    "    logging.info(f\"剪枝后的预测类别：{dt.predict(x_test)}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    formatter = '[%(asctime)s] - %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(level=logging.INFO,  # 如果需要查看简略信息可将该参数改为logging.INFO\n",
    "                        format=formatter,  # 关于Logging模块的详细使用可参加文章 https://mp.weixin.qq.com/s/cvO6hCiHMJqC4-4AuUlydw\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                        handlers=[logging.StreamHandler(sys.stdout)])\n",
    "    test_decision_tree_pruning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4672f3-9cff-4617-b219-0b164d344cc8",
   "metadata": {},
   "source": [
    "## 实验总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d5271b-6756-448a-8dd6-69764ebbaa7e",
   "metadata": {},
   "source": [
    "在本节实验中，我们详细介绍了如何从零实现ID3决策树算法，包括节点定义、决策树构建、剪枝以及垃圾邮件分类的内容；同时也对各个部分的实现进行了详细的测试，以便更加清晰地掌握整个决策树的实现过程。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
