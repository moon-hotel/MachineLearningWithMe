{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f855fec9-69a6-48c6-9be5-61fc1fe30528",
   "metadata": {},
   "source": [
    "# 【实验】第7.3节朴素贝叶斯实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f68b24-7990-49c8-8599-0132b50a7549",
   "metadata": {},
   "source": [
    "## 实验介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0326ece-46d3-4918-9d04-bcfb877e4e18",
   "metadata": {},
   "source": [
    "在本节实验中，我们将详细逐一介绍如何从零实现朴素贝叶斯实现算法、文本向量化方法、垃圾邮件分类等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b38b112-4700-4738-a46d-fa94ea44806d",
   "metadata": {},
   "source": [
    "### 知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89f9fb-a4d7-49ef-88e8-edfd10634e65",
   "metadata": {},
   "source": [
    "- 朴素贝叶斯实现算法\n",
    "- LabelBinarizer使用\n",
    "- 文本向量化实现\n",
    "- 垃圾邮件分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd4b11-236b-4955-a7a1-f76e61a861c9",
   "metadata": {},
   "source": [
    "## 1. 构造示例数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325cf487-ac69-4901-a59c-61fde5592d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "import logging\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "formatter = '[%(asctime)s] - %(levelname)s: %(message)s'\n",
    "logging.basicConfig(level=logging.DEBUG,  # 如果需要查看详细信息可将该参数改为logging.DEBUG\n",
    "                    format=formatter,  # \n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    handlers=[logging.StreamHandler(sys.stdout)])\n",
    "\n",
    "def load_simple_data():\n",
    "    x = np.array([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                  [1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n",
    "                  [2, 1, 1, 2, 2, 2, 0, 2, 2, 0, 0, 2, 2, 1, 1]]).transpose()\n",
    "    y = np.array([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa876aa4-54aa-4446-a4c1-5c30f685d3b0",
   "metadata": {},
   "source": [
    "## 2. 实现朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0218191-aefd-49fe-a364-8f84f5818219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCategoricalNB(object):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        alpha: 平滑项，默认为1，即拉普拉斯平滑\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self._ALPHA_MIN = 1e-10\n",
    "\n",
    "    def _check_alpha(self):\n",
    "        \"\"\"\n",
    "        检查 alpha的取值\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if np.min(self.alpha) < self._ALPHA_MIN:\n",
    "            self.alpha = np.maximum(self.alpha, self._ALPHA_MIN)\n",
    "\n",
    "    def _init_counters(self):\n",
    "        \"\"\"\n",
    "        初始化计数器\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.class_count_ = np.zeros(self.n_classes, dtype=np.float64)\n",
    "        # shape: [n_classes, ] 后续用来记录每个类别下的样本数\n",
    "        # 每个维度表示每个类别的样本数量，e.g. [2,2,3] 表示0,1,2这三个类别的样本数分别是2,2,3\n",
    "        # 其作用是后续用来计算每个类别的先验概率\n",
    "        self.category_count_ = [np.zeros((self.n_classes, 0))\n",
    "                                for _ in range(self.n_features_)]\n",
    "        # n_features_个元素（array()），目前每个元素的shape是[n_classes,0]\n",
    "        # 后续每个元素的shape将会更新为[n_classes,len(X_i)], len(X_i)表示X_i这个特征的取值情况数量\n",
    "        # 目的是用来记录在各个类别下每个特征变量中各种取值情况的数量\n",
    "        # 例如category_count_[i][j][k]为10 表示含义就是特征i在类别j下特征取值为k的样本数量为10\n",
    "\n",
    "    def _count(self, X, Y):\n",
    "        \"\"\"\n",
    "        对数据集每个特征维度下的取值情况进行统计\n",
    "        :param X: shape [n_samples,n_features]\n",
    "        :param Y: shape [n_samples,]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        def _update_cat_count(X_feature, Y, cat_count, n_classes):\n",
    "            \"\"\"\n",
    "            对每一列特征进行统计处理\n",
    "            :param X_feature:  模型输入的某一列特征X_i, shape: [n_samples,]\n",
    "            :param Y:    one-hot 形式标签 shape: [n_samples,n_classes]\n",
    "            :param cat_count:   shape: [n_classes,len(X_i)], len(X_i)表示X_i这个特征的取值情况数量\n",
    "            :param n_classes:   n_classes,数据集的类别数量\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            for j in range(n_classes):  # 遍历每个类别\n",
    "                mask = Y[:, j].astype(bool)  # 取每个类别下对应样本的索引\n",
    "                counts = np.bincount(X_feature[mask])  # 统计当前类别下，特征X_feature中各个取值下的数量\n",
    "                # np.bincount的作用的是统计每个值出现的次数，例如\n",
    "                # counts = np.bincount(np.array([0, 3, 5, 1, 4, 4]))\n",
    "                # print(counts) [1 1 0 1 2 1]\n",
    "                # 表示[0, 3, 5, 1, 4, 4]中0,1,2,3,4,5这个6个值的出现的频次分别是1,1,0,1,2,1\n",
    "                indices = np.nonzero(counts)[0]\n",
    "                cat_count[j, indices] += counts[indices]\n",
    "                # cat_count[i,k]表示第i个类别下，特征X_feature第k个取值情况的数量\n",
    "\n",
    "        self.class_count_ += Y.sum(axis=0)  # Y: shape(n,n_classes)   Y.sum(): shape(n_classes,)\n",
    "        # self.class_count_的shape是(n_classes,)  每个维度表示每个类别的样本数量\n",
    "        # e.g. [2,2,3] 表示0,1,2这三个类别的样本数分别是2,2,3\n",
    "        logging.debug(f\"数据集X为:\\n{X}\")\n",
    "        logging.debug(f\"标签Y为:\\n{Y}\")\n",
    "        logging.debug(f\"每个类别下的样本数class_count_(n_classes,): {self.class_count_}\")\n",
    "        self.n_categories_ = X.max(axis=0) + 1\n",
    "        # 统计每个特征维度的 取值数量（因为特征取值是从0开始的所以后面加了1）,e.g.  [3 3 3 3]，表示四个维度的取值均有3中情况\n",
    "        logging.debug(f\"每个特征的取值种数n_categories_:{self.n_categories_}\")\n",
    "\n",
    "        for i in range(self.n_features_):  # 遍历每个特征\n",
    "            X_feature = X[:, i]  # 取每一列的特征\n",
    "            self.category_count_[i] = np.pad(self.category_count_[i],\n",
    "                                             [(0, 0), (0, self.n_categories_[i])],\n",
    "                                             'constant')  # shape: [n_classes,n_categories_[i]]\n",
    "            # np.pad(a,((1,2),(3,4)),'constant') 含义是在a的第一个维度（行）的上面和下面各填充1行和2行0，\n",
    "            # 在a的第二个维度（列）的左边和右边各填充3列和4列0\n",
    "            # 在原始category_count_[i]的基础上，追加n_categories_[i]列全为0的值，\n",
    "            # 因为category_count_[i]初始化式时的shape为[n_classes,0]\n",
    "            _update_cat_count(X_feature, Y,\n",
    "                              self.category_count_[i],\n",
    "                              self.n_classes)\n",
    "        # category_count_为一个包含有n_features个元素的列表\n",
    "        # category_count_[i][j][k]为10 表示含义就是特征i个在类别j下特征取值为k的样本数量为10\n",
    "        logging.debug(f\"各个特征每个取值的数量分布（未平滑处理） category_count_:\\n {self.category_count_}\")\n",
    "\n",
    "    def _update_feature_prob(self):\n",
    "        \"\"\"\n",
    "        计算条件概率\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        feature_prob = []\n",
    "        for i in range(self.n_features_):  # 遍历 每一个特征\n",
    "\n",
    "            # 以下两行是sklearn中的平滑处理方式\n",
    "            # smoothed_cat_count = self.category_count_[i] + self.alpha  # 平滑处理\n",
    "            # smoothed_class_count = smoothed_cat_count.sum(axis=1)\n",
    "            # 以下两行是文中的平滑处理方式\n",
    "            smoothed_cat_count = self.category_count_[i] + self.alpha\n",
    "            smoothed_class_count = self.category_count_[i].sum(axis=1) + self.category_count_[i].shape[1] * self.alpha\n",
    "\n",
    "            cond_prob = smoothed_cat_count / smoothed_class_count.reshape(-1, 1)\n",
    "            feature_prob.append(cond_prob)\n",
    "            logging.debug(f\"第{i}个特征在各类别下各个特征取值的条件概率为: \\n{cond_prob}\")\n",
    "            logging.debug(f\"第{i}个特征在各类别下各个特征取值数为: \\n{smoothed_cat_count}\")\n",
    "        self.feature_prob_ = feature_prob\n",
    "        # feature_prob_ 为一个包含有n_features_个元素的列表，每个元素的shape为 (self.n_classes,特征取值数)\n",
    "\n",
    "    def _update_class_prior(self):\n",
    "        \"\"\"\n",
    "        计算先验概率\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        logging.debug(f\"n_classes:{self.n_classes}\")\n",
    "        logging.debug(f\"class_count_:{self.class_count_}\")\n",
    "        # empirical prior, with sample_weight taken into account\n",
    "        self.class_prior_ = (self.class_count_ + self.alpha) / (self.class_count_.sum() + self.n_classes * self.alpha)\n",
    "        logging.debug(f\"计算每个类别的先验概率class_prior_:{self.class_prior_}\")\n",
    "\n",
    "    def _joint_likelihood(self, X):\n",
    "        \"\"\"\n",
    "        计算后验概率\n",
    "        :param X: shape: [n_samples,n_features]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if not X.shape[1] == self.n_features_:\n",
    "            raise ValueError(\"Expected input with %d features, got %d instead\"\n",
    "                             % (self.n_features_, X.shape[1]))\n",
    "        jll = np.ones((X.shape[0], self.class_count_.shape[0]))  # 用来累积条件概率\n",
    "        for i in range(self.n_features_):\n",
    "            indices = X[:, i]  # 取对应的每一列特征\n",
    "            if self.feature_prob_[i].shape[1] <= indices.max():\n",
    "                raise IndexError(f\"测试集中的第{i}个特征维度的取值情况\"\n",
    "                                 f\" {indices.max()} 超出了训练集中该维度的取值情况！\")\n",
    "            jll *= self.feature_prob_[i][:, indices].T  # 取每个特征取值下对应的条件概率，并进行累乘\n",
    "            # feature_prob_[i][:, indices]  表示第i个特征下，取对应特征取值对应的条件概率\n",
    "            # feature_prob_[i]的shape为 (n_classes,特征取值数),\n",
    "            # feature_prob_[i][j][k]表示特征[i]在类别j下，取值为k时的概率\n",
    "        total_ll = jll * self.class_prior_  # 条件概率乘以先验概率即得到后验概率\n",
    "        return total_ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : shape: [n_samples,n_features]\n",
    "        y : shape [n_samples,]\n",
    "        \"\"\"\n",
    "        self.n_features_ = X.shape[1]\n",
    "        labelbin = LabelBinarizer()  # 将标签转化为one-hot形式\n",
    "        Y = labelbin.fit_transform(y)  # one-hot 形式标签 shape: [n,n_classes]\n",
    "        self.classes_ = labelbin.classes_  # 原始标签类别 shape: [n_classes,]\n",
    "        if Y.shape[1] == 1:  # 当数据集为二分类时fit_transform处理后的结果并不是one-hot形式\n",
    "            Y = np.concatenate((1 - Y, Y), axis=1)  # 改变为one-hot形式\n",
    "        self.n_classes = Y.shape[1]  # 数据集的类别数量\n",
    "        self._init_counters()  # 初始化计数器\n",
    "        self._count(X, Y)  # 对各个特征的取值情况进行计数，以计算条件概率等\n",
    "        self._check_alpha()  # 检查平滑\n",
    "        self._update_class_prior()\n",
    "        self._update_feature_prob()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, with_prob=False):\n",
    "        \"\"\"\n",
    "        极大化概率进行预测\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : shape: [n_samples,n_features]\n",
    "        \"\"\"\n",
    "        from scipy.special import softmax\n",
    "        jll = self._joint_likelihood(X)\n",
    "        logging.debug(f\"样本预测原始概率为：{jll}\")\n",
    "        y_pred = self.classes_[np.argmax(jll, axis=1)]\n",
    "        if with_prob:\n",
    "            prob = softmax(jll)\n",
    "            return y_pred, prob\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e065c9ae-6bcb-4b58-8063-7462df0b60ff",
   "metadata": {},
   "source": [
    "## 3.使用示例数据进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0d826ad-a0cc-4072-8b84-d65036ad37bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-12 19:40:11] - INFO: My Bayes 运行结果：\n",
      "[2024-06-12 19:40:11] - DEBUG: 数据集X为:\n",
      "[[0 1 2]\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [0 0 2]\n",
      " [0 1 2]\n",
      " [0 0 2]\n",
      " [0 0 0]\n",
      " [1 0 2]\n",
      " [1 0 2]\n",
      " [1 0 0]\n",
      " [1 1 0]\n",
      " [1 1 2]\n",
      " [1 1 2]\n",
      " [1 0 1]\n",
      " [1 0 1]]\n",
      "[2024-06-12 19:40:11] - DEBUG: 标签Y为:\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]]\n",
      "[2024-06-12 19:40:12] - DEBUG: 每个类别下的样本数class_count_(n_classes,): [ 5. 10.]\n",
      "[2024-06-12 19:40:12] - DEBUG: 每个特征的取值种数n_categories_:[2 2 3]\n",
      "[2024-06-12 19:40:12] - DEBUG: 各个特征每个取值的数量分布（未平滑处理） category_count_:\n",
      " [array([[4., 1.],\n",
      "       [3., 7.]]), array([[4., 1.],\n",
      "       [4., 6.]]), array([[1., 1., 3.],\n",
      "       [2., 3., 5.]])]\n",
      "[2024-06-12 19:40:12] - DEBUG: n_classes:2\n",
      "[2024-06-12 19:40:12] - DEBUG: class_count_:[ 5. 10.]\n",
      "[2024-06-12 19:40:12] - DEBUG: 计算每个类别的先验概率class_prior_:[0.33333333 0.66666667]\n",
      "[2024-06-12 19:40:12] - DEBUG: 第0个特征在各类别下各个特征取值的条件概率为: \n",
      "[[0.8 0.2]\n",
      " [0.3 0.7]]\n",
      "[2024-06-12 19:40:12] - DEBUG: 第0个特征在各类别下各个特征取值数为: \n",
      "[[4. 1.]\n",
      " [3. 7.]]\n",
      "[2024-06-12 19:40:12] - DEBUG: 第1个特征在各类别下各个特征取值的条件概率为: \n",
      "[[0.8 0.2]\n",
      " [0.4 0.6]]\n",
      "[2024-06-12 19:40:12] - DEBUG: 第1个特征在各类别下各个特征取值数为: \n",
      "[[4. 1.]\n",
      " [4. 6.]]\n",
      "[2024-06-12 19:40:12] - DEBUG: 第2个特征在各类别下各个特征取值的条件概率为: \n",
      "[[0.2 0.2 0.6]\n",
      " [0.2 0.3 0.5]]\n",
      "[2024-06-12 19:40:12] - DEBUG: 第2个特征在各类别下各个特征取值数为: \n",
      "[[1. 1. 3.]\n",
      " [2. 3. 5.]]\n",
      "[2024-06-12 19:40:12] - DEBUG: 样本预测原始概率为：[[0.01066667 0.024     ]]\n",
      "[2024-06-12 19:40:12] - INFO: (array([1]), array([[0.49666672, 0.50333328]]))\n",
      "[2024-06-12 19:40:12] - INFO: CategoricalNB 运行结果：\n",
      "[2024-06-12 19:40:12] - INFO: [1]\n",
      "[2024-06-12 19:40:12] - INFO: [[0.30769231 0.69230769]]\n"
     ]
    }
   ],
   "source": [
    "def test_naive_bayes():\n",
    "    x, y = load_simple_data()\n",
    "    logging.info(f\"My Bayes 运行结果：\")\n",
    "    model = MyCategoricalNB(alpha=0)\n",
    "    model.fit(x, y)\n",
    "    logging.info(model.predict(np.array([[0, 1, 0]]), with_prob=True))\n",
    "    logging.info(f\"CategoricalNB 运行结果：\")\n",
    "    model = CategoricalNB(alpha=0)\n",
    "    model.fit(x, y)\n",
    "    logging.info(model.predict(np.array([[0, 1, 0]])))\n",
    "    logging.info(model.predict_proba(np.array([[0, 1, 0]])))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    test_naive_bayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01b627-dec8-46e4-bd6d-f243a68553b1",
   "metadata": {},
   "source": [
    "## 4.向量化实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3404e8ac-0dd6-46ab-b4ef-55b4a0968fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VectWithoutFrequency(object):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, top_k_words=500):\n",
    "        self.top_k_words = top_k_words\n",
    "\n",
    "    def _get_vocab(self, raw_documents):\n",
    "\n",
    "        c = Counter()\n",
    "        for sample in raw_documents:\n",
    "            words_list = sample.split()\n",
    "            for x in words_list:\n",
    "                if len(x) > 1 and x != '\\r\\n':\n",
    "                    c[x] += 1\n",
    "        # ---------词频统计构造词表------------------\n",
    "        vocab = []\n",
    "        for (k, v) in c.most_common(self.top_k_words):  # 输出词频最高的前top_k_words个词\n",
    "            vocab.append(k)\n",
    "        return vocab\n",
    "\n",
    "    def fit_transform(self, raw_documents):\n",
    "        \"\"\"\n",
    "        拟合\n",
    "        :param raw_documents: 原始样本，list， 每个元素为分词后的样本\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.fit(raw_documents)\n",
    "        x = self.transform(raw_documents)\n",
    "        return x\n",
    "\n",
    "    def transform(self, raw_documents):\n",
    "        \"\"\"\n",
    "        :param raw_documents:\n",
    "        :return:\n",
    "        e.g.\n",
    "        s = ['文本 分词 工具 可 用于 对 文本 进行 分词 处理', '常见 的 用于 处理 文本 的 分词 处理 工具 有 很多']\n",
    "        vect = VectWithoutFrequency()\n",
    "          x = vect.fit_transform(s)\n",
    "          vect.vocab: ['文本', '分词', '处理', '工具', '用于', '进行', '常见', '很多']\n",
    "        x:\n",
    "          [[1, 1, 1, 1, 1, 1, 0, 0],\n",
    "           [1, 1, 1, 1, 1, 0, 1, 1]]\n",
    "        \"\"\"\n",
    "        x_vec = []\n",
    "        for item in raw_documents:\n",
    "            tmp = [0] * len(self.vocabulary)\n",
    "            for i, w in enumerate(self.vocabulary):\n",
    "                if w in item:\n",
    "                    tmp[i] = 1\n",
    "            x_vec.append(tmp)\n",
    "        return np.array(x_vec)\n",
    "\n",
    "    def fit(self, raw_documents):\n",
    "        self.vocabulary = self._get_vocab(raw_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc08bc16-2767-4787-b8c2-02e97b41b2e5",
   "metadata": {},
   "source": [
    "## 5.垃圾邮件分类测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2d5045a-c533-4ab0-93c1-38053d90a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string, sep=\" \"):\n",
    "    \"\"\"\n",
    "    该函数的作用是去掉一个字符串中的所有非中文字符\n",
    "    :param string: 输入必须是字符串类型\n",
    "    :param sep: 表示去掉的部分用什么填充，默认为一个空格\n",
    "    :return: 返回处理后的字符串\n",
    "    example:\n",
    "    s = \"祝你2018000国庆快乐！\"\n",
    "    print(clean_str(s))# 祝你 国庆快乐\n",
    "    print(clean_str(s,sep=\"\"))# 祝你国庆快乐\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^\\u4e00-\\u9fff]\", sep, string)\n",
    "    string = re.sub(r\"\\s{1,}\", sep, string)  # 若有空格，则最多只保留1个宽度\n",
    "    return string.strip()\n",
    "    \n",
    "def load_data():\n",
    "    x, y = load_cut_spam()\n",
    "    x_train, x_test, y_train, y_test \\\n",
    "        = train_test_split(x, y, test_size=0.3, random_state=2020)\n",
    "    vect = VectWithoutFrequency(top_k_words=1000)\n",
    "    x_train = vect.fit_transform(x_train)\n",
    "    x_test = vect.transform(x_test)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def load_spam():\n",
    "    \"\"\"\n",
    "    载入原始文本\n",
    "    :return: x为一个list，每个元素为一个样本\n",
    "             y为一个list，每个元素为样本对应的标签\n",
    "    \"\"\"\n",
    "    data_spam_dir = os.path.join(os.path.dirname(os.path.dirname('./')), 'data')\n",
    "    def load_spam_data(file_path=None):\n",
    "        texts = []\n",
    "        with open(file_path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip('\\n')\n",
    "                texts.append(clean_str(line))\n",
    "        return texts\n",
    "\n",
    "    x_pos = load_spam_data(file_path=os.path.join(data_spam_dir, 'ham_5000.utf8'))\n",
    "    x_neg = load_spam_data(file_path=os.path.join(data_spam_dir, 'spam_5000.utf8'))\n",
    "    y_pos, y_neg = [1] * len(x_pos), [0] * len(x_neg)\n",
    "    x, y = x_pos + x_neg, y_pos + y_neg\n",
    "    return x, y\n",
    "    \n",
    "def load_cut_spam():\n",
    "    \"\"\"\n",
    "    :return: ['中信   国际   电子科技 有限公司 推出 新 产品   升职 步步高',\n",
    "             '搜索 文件   看 是否 不 小心 拖 到 某个 地方 了',....]\n",
    "    \"\"\"\n",
    "    x, y = load_spam()\n",
    "    x_cut = []\n",
    "    for text in x:\n",
    "        seg_list = jieba.cut(text, cut_all=False)\n",
    "        tmp = \" \".join(seg_list)\n",
    "        x_cut.append(tmp)\n",
    "    return x_cut, y\n",
    "\n",
    "def test_spam_classification():\n",
    "    x_train, x_test, y_train, y_test = load_data()\n",
    "    model = MyCategoricalNB(alpha=1.0)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    logging.info(f\"MyCategoricalNB 运行结果：\")\n",
    "    logging.info(classification_report(y_test, y_pred))\n",
    "\n",
    "    model = CategoricalNB()\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    logging.info(f\"CategoricalNB 运行结果：\")\n",
    "    logging.info(classification_report(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df856b-9182-48ab-8109-12d143f51153",
   "metadata": {},
   "source": [
    "## 6.运行结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25655b38-787b-474d-b983-6c6db7fe7bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-12 19:39:49] - INFO: MyCategoricalNB 运行结果：\n",
      "[2024-06-12 19:39:49] - INFO:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      1504\n",
      "           1       0.97      0.98      0.97      1497\n",
      "\n",
      "    accuracy                           0.97      3001\n",
      "   macro avg       0.97      0.97      0.97      3001\n",
      "weighted avg       0.97      0.97      0.97      3001\n",
      "\n",
      "[2024-06-12 19:39:49] - INFO: CategoricalNB 运行结果：\n",
      "[2024-06-12 19:39:49] - INFO:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      1504\n",
      "           1       0.97      0.98      0.97      1497\n",
      "\n",
      "    accuracy                           0.97      3001\n",
      "   macro avg       0.97      0.97      0.97      3001\n",
      "weighted avg       0.97      0.97      0.97      3001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_spam_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53401513-4c33-4685-ac13-4caacaba17c7",
   "metadata": {},
   "source": [
    "## 实验总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f927e4-84c2-4814-9b55-4b511a58b358",
   "metadata": {},
   "source": [
    "在本节实验中，我们详细介绍了如何从零实现朴素贝叶斯实现算法、文本向量化方法、以及基于贝叶斯算法的垃圾邮件分类等。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
